import matplotlib.pyplot as plt
import numpy as np

# Extract data
epochs = range(100)
# reward_losses = [22.4024, 13.6330, 8.9887, 7.2082, 5.4321, 5.1535, 4.1839, 3.3806, 3.4301, 2.3752,
#                  2.2692, 1.8545, 1.3879, 0.9981, 1.2466, 0.9314, 1.2630, 0.7969, 0.8418, 0.7419,
#                  0.4619, 0.6012, 0.5930, 0.5269, 0.2463, 0.3946, 0.3674, 0.3171, 0.3649, 0.2515,
#                  0.0858, 0.2885, 0.1355, 0.2376, 0.1753, 0.1714, 0.1680, 0.1369, 0.2252, 0.1242,
#                  0.1230, 0.0933, 0.0583, 0.0885, 0.0437, 0.0650, 0.0919, 0.0931, 0.0749, 0.1221,
#                  0.0705, 0.0538, 0.0513, 0.0739, 0.0947, 0.0653, 0.0381, 0.1195, 0.1178, 0.0283,
#                  0.0768, 0.0106, 0.0558, 0.0374, 0.0615, 0.0390, 0.0309, 0.0188, 0.0359, 0.0330,
#                  0.0276, 0.0349, 0.0361, 0.0513, 0.0944, 0.0612, 0.0425, 0.0408, 0.0475, 0.0449,
#                  0.0438, 0.0028, 0.0492, 0.0830, 0.0387, 0.0355, 0.0690, 0.0189, 0.0474, 0.0396,
#                  0.0460, 0.0441, 0.0546, 0.0449, 0.0706, 0.0585, 0.1107, 0.0410, 0.1008, 0.0229]

# policy_losses = [3.1644, 3.1631, 3.1619, 3.1605, 3.1592, 3.1577, 3.1561, 3.1543, 3.1524, 3.1504,
#                  3.1481, 3.1457, 3.1432, 3.1405, 3.1377, 3.1348, 3.1319, 3.1290, 3.1262, 3.1233,
#                  3.1203, 3.1171, 3.1137, 3.1099, 3.1060, 3.1018, 3.0976, 3.0932, 3.0889, 3.0846,
#                  3.0803, 3.0759, 3.0716, 3.0673, 3.0629, 3.0586, 3.0542, 3.0498, 3.0454, 3.0410,
#                  3.0366, 3.0322, 3.0279, 3.0237, 3.0195, 3.0154, 3.0113, 3.0072, 3.0029, 2.9986,
#                  2.9941, 2.9895, 2.9850, 2.9804, 2.9761, 2.9720, 2.9680, 2.9641, 2.9601, 2.9562,
#                  2.9524, 2.9487, 2.9451, 2.9415, 2.9379, 2.9342, 2.9305, 2.9268, 2.9231, 2.9195,
#                  2.9158, 2.9122, 2.9085, 2.9049, 2.9013, 2.8977, 2.8943, 2.8909, 2.8875, 2.8842,
#                  2.8809, 2.8775, 2.8741, 2.8707, 2.8673, 2.8639, 2.8606, 2.8575, 2.8546, 2.8517,
#                  2.8488, 2.8459, 2.8429, 2.8399, 2.8369, 2.8341, 2.8314, 2.8287, 2.8260, 2.8234]


reward_losses = [90.1142, 40.8754, 35.9160, 21.9665, 12.5449, 6.2543, 6.6917, 4.8870, 3.6633, 3.2635,
               3.1420, 2.2530, 2.6991, 2.3830, 2.5927, 1.9157, 1.5029, 1.3195, 1.1300, 1.1124,
               1.2674, 0.7828, 0.5696, 0.9340, 0.9302, 0.7071, 0.6395, 0.6993, 0.7954, 0.8439,
               0.7374, 0.6245, 0.8185, 0.5217, 0.2934, 0.2833, 0.5935, 0.3826, 0.4457, 0.4687,
               0.6294, 0.7198, 0.4447, 0.2923, 0.9441, 0.7210, 0.6035, 0.6532, 0.7525, 0.5470,
               0.6606, 0.6510, 0.3721, 0.4742, 0.3186, 0.4312, 0.6441, 0.4752, 0.5026, 0.4076,
               0.4771, 0.5625, 0.5717, 0.5468, 0.5066, 0.4207, 0.4103, 0.6441, 0.7292, 0.6038,
               0.6127, 0.4211, 0.4971, 0.8309, 0.6673, 0.7129, 0.9323, 0.9074, 0.9628, 0.4946,
               0.5255, 1.0483, 0.5225, 0.6252, 0.4634, 0.7255, 0.8658, 0.8328, 0.6085, 0.7814,
               0.7795, 0.9100, 1.0838, 0.8641, 0.6456, 0.7507, 0.9839, 0.9554, 1.0055, 0.7754]

policy_losses = [2.2275, 2.2212, 2.2149, 2.2080, 2.2003, 2.1918, 2.1823, 2.1715, 2.1590, 2.1451,
               2.1294, 2.1119, 2.0928, 2.0721, 2.0505, 2.0292, 2.0098, 1.9951, 1.9877, 1.9897,
               1.9987, 2.0060, 2.0060, 1.9992, 1.9889, 1.9787, 1.9706, 1.9653, 1.9622, 1.9604,
               1.9588, 1.9568, 1.9542, 1.9506, 1.9463, 1.9414, 1.9362, 1.9311, 1.9263, 1.9222,
               1.9188, 1.9161, 1.9140, 1.9123, 1.9108, 1.9092, 1.9074, 1.9053, 1.9031, 1.9009,
               1.8989, 1.8970, 1.8952, 1.8935, 1.8919, 1.8904, 1.8889, 1.8874, 1.8861, 1.8848,
               1.8835, 1.8822, 1.8809, 1.8795, 1.8779, 1.8763, 1.8744, 1.8723, 1.8700, 1.8676,
               1.8651, 1.8623, 1.8596, 1.8569, 1.8542, 1.8516, 1.8491, 1.8464, 1.8436, 1.8406,
               1.8373, 1.8338, 1.8302, 1.8268, 1.8233, 1.8199, 1.8164, 1.8128, 1.8092, 1.8056,
               1.8021, 1.7987, 1.7953, 1.7921, 1.7890, 1.7861, 1.7833, 1.7807, 1.7782, 1.7759]

# Create figure
plt.figure(figsize=(12, 6))

# Plot both losses
plt.plot(epochs, reward_losses, 'r-', label='Reward Loss', alpha=0.7)
plt.plot(epochs, policy_losses, 'b-', label='Policy Loss', alpha=0.7)

# Add second y-axis
ax1 = plt.gca()
ax2 = ax1.twinx()
ax2.plot(epochs, reward_losses, 'r-', alpha=0)

# Customize the plot
plt.title('Training Losses over Epochs', fontsize=14, pad=20)
ax1.set_xlabel('Epochs', fontsize=12)
ax1.set_ylabel('Policy Loss', color='b', fontsize=12)
ax2.set_ylabel('Reward Loss', color='r', fontsize=12)

# Set y-axis colors
ax1.tick_params(axis='y', labelcolor='b')
ax2.tick_params(axis='y', labelcolor='r')

# Add grid
ax1.grid(True, alpha=0.3)

# Add legend
lines1, labels1 = ax1.get_legend_handles_labels()
ax1.legend(lines1, labels1, loc='upper right')

plt.tight_layout()
plt.show()

# Print analysis statistics
print("\nLoss Analysis:")
print("\nReward Loss:")
print(f"Initial: {reward_losses[0]:.4f}")
print(f"Final: {reward_losses[-1]:.4f}")
print(f"Reduction: {((reward_losses[0] - reward_losses[-1])/reward_losses[0]*100):.2f}%")
print(f"Mean: {np.mean(reward_losses):.4f}")
print(f"Std: {np.std(reward_losses):.4f}")

print("\nPolicy Loss:")
print(f"Initial: {policy_losses[0]:.4f}")
print(f"Final: {policy_losses[-1]:.4f}")
print(f"Reduction: {((policy_losses[0] - policy_losses[-1])/policy_losses[0]*100):.2f}%")
print(f"Mean: {np.mean(policy_losses):.4f}")
print(f"Std: {np.std(policy_losses):.4f}")